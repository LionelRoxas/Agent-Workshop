{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter AI Agent Workshop\n",
        "\n",
        "#### **Skills: OpenAI, Groq, Llama, OpenRouter**\n",
        "\n",
        "## **To Get Started:**\n",
        "1. [Get your Groq API Key](https://console.groq.com/keys)\n",
        "2. [Get your OpenRouter API Key](https://openrouter.ai/settings/keys)\n",
        "3. [Get your OpenAI API Key](https://platform.openai.com/api-keys)\n",
        "\n",
        "\n",
        "### **Interesting Reads**\n",
        "- [Sam Altman's Blog Post: The Intelligence Age](https://ia.samaltman.com/)\n",
        "- [What LLMs cannot do](https://ehudreiter.com/2023/12/11/what-llms-cannot-do/)\n",
        "- [Chain of Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n",
        "- [Why ChatGPT can't count the number of r's in the word strawberry](https://prompt.16x.engineer/blog/why-chatgpt-cant-count-rs-in-strawberry)\n",
        "\n",
        "\n",
        "## During the Workshop\n",
        "- [Any code shared during the workshop will be posted here](https://docs.google.com/document/d/1hPBJt_4Ihkj6v667fWxVjzwCMS4uBPdYlBLd2IqkxJ0/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "nO8gHDbSAa4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries"
      ],
      "metadata": {
        "id": "Pt_SMGlvocqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai groq"
      ],
      "metadata": {
        "id": "eKvKwO8XAnPt",
        "outputId": "cb7699d2-fca3-44d8-c599-bcf6a6ad0b50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.50.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Groq, OpenRouter, & OpenAI clients"
      ],
      "metadata": {
        "id": "GjcgEeFaof1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pHbjDU_L__Vd"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "import json\n",
        "from typing import List, Dict, Any, Callable\n",
        "import ast\n",
        "import io\n",
        "import sys\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "openrouter_api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "os.environ['OPENROUTER_API_KEY'] = openrouter_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
        ")\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    base_url=\"https://api.openai.com/v1\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions to easily query and compare responses from OpenAI, Groq, and OpenRouter"
      ],
      "metadata": {
        "id": "AZF6uLpooj-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(client, prompt, openai_model=\"o1-preview\", json_mode=False):\n",
        "\n",
        "    if client == \"openai\":\n",
        "\n",
        "        kwargs = {\n",
        "            \"model\": openai_model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "        }\n",
        "\n",
        "        if json_mode:\n",
        "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        response = openai_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    elif client == \"groq\":\n",
        "\n",
        "        try:\n",
        "            models = [\"llama-3.1-8b-instant\", \"llama-3.1-70b-versatile\", \"llama3-70b-8192\", \"llama3-8b-8192\", \"gemma2-9b-it\"]\n",
        "\n",
        "            for model in models:\n",
        "\n",
        "                try:\n",
        "                    kwargs = {\n",
        "                        \"model\": model,\n",
        "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "                    }\n",
        "                    if json_mode:\n",
        "                        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "                    response = groq_client.chat.completions.create(**kwargs)\n",
        "\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "            kwargs = {\n",
        "                \"model\": \"meta-llama/llama-3.1-8b-instruct:free\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "            }\n",
        "\n",
        "            if json_mode:\n",
        "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "            response = openrouter_client.chat.completions.create(**kwargs)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid client: {client}\")\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def evaluate_responses(prompt, reasoning_prompt=False, openai_model=\"gpt-3.5-turbo\"):\n",
        "\n",
        "    if reasoning_prompt:\n",
        "        prompt = f\"{prompt}\\n\\n{reasoning_prompt}.\"\n",
        "\n",
        "    # openai_response = get_llm_response(\"openai\", prompt, openai_model)\n",
        "    groq_response = get_llm_response(\"groq\", prompt)\n",
        "\n",
        "    #print(f\"OpenAI Response: {openai_response}\")\n",
        "    print(f\"\\n\\nGroq Response: {groq_response}\")"
      ],
      "metadata": {
        "id": "BNYyDCNuAhbj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"How many r's are in the word 'strawberry'?\"\n",
        "evaluate_responses(prompt1)"
      ],
      "metadata": {
        "id": "j9V03RXNBd6j",
        "outputId": "024e71ea-b453-4813-84e3-187c41a70877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Groq Response: In the word \"strawberry\", there are 2 R's.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYaiRrHwbwMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mq1YkfFeCRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgyXx8M9C0RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Architecture"
      ],
      "metadata": {
        "id": "0w8IlZq49PAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![](https://mermaid.ink/img/pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW?type=png)](https://mermaid.live/edit#pako:eNqFUslugzAQ_ZWRz8kPILUVCZClVdQmqdTK5ODCFFDAjry0iSD_XmOTJodK5WS_Zd7M4JZkIkcSkEKyQwnbKOVgvzf6qlDCi0F52sF4fA-hJ0IaGi24aIRREBbItacn9LlmnKPced2kR7s1aiO5AmU-NFN71cG0fRLiALqUwhQlbAbi7F1TVyuia2RKXItFDo5pmOnqBo5dhgcDmKFlmEaY2oE6SOgvwHgO8REzM5B_2n1kxYsOZtSrxSUocfkzf5m5y5zGX6w27Cqau3IDOpTU8gR3kLBa2Y4W7QqP-jLywzDywtnesd_NLbISHSxpUnFWQ8jVt_0b8VFLll0Tl66TR-q3DLfaf3vaSoMukYxIg7JhVW4fQdvbUqJLbDAlgT3mTO5TkvKz1TG7ks2JZyTQ1j0i5pDb9UYVs2-nIcFnP-YFjfPKNjqA5x9CM8YW)\n",
        "\n",
        "\n",
        "![agent_architecture_v2](https://github.com/user-attachments/assets/a65b6db9-bef1-4579-aed3-01444ce40544)"
      ],
      "metadata": {
        "id": "RxpUp2KED9hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To create our AI Agent, we will define the following functions:\n",
        "\n",
        "1. **Planner:** This function takes a user's query and breaks it down into smaller, manageable subtasks. It returns these subtasks as a list, where each one is either a reasoning task or a code generation task.\n",
        "\n",
        "2. **Reasoner:** This function provides reasoning on how to complete a specific subtask, considering both the overall query and the results of any previous subtasks. It returns a short explanation on how to proceed with the current subtask.\n",
        "\n",
        "3. **Actioner:** Based on the reasoning provided for a subtask, this function decides whether the next step requires generating code or more reasoning. It then returns the chosen action and any necessary details to perform it.\n",
        "\n",
        "4. **Evaluator:** This function checks if the result of the current subtask is reasonable and aligns with the overall goal. It returns an evaluation of the result and indicates whether the subtask needs to be retried.\n",
        "\n",
        "5. **generate_and_execute_code:** This function generates and executes Python code based on a given prompt and memory of previous steps. It returns both the generated code and its execution result.\n",
        "\n",
        "6. **executor:** Depending on the action decided by the \"actioner,\" this function either generates and executes code or returns reasoning. It handles the execution of tasks based on the action type.\n",
        "\n",
        "7. **final_answer_extractor:** After all subtasks are completed, this function gathers the results from previous steps to extract and provide the final answer to the user's query.\n",
        "\n",
        "8. **autonomous_agent:** This is the main function that coordinates the process of answering the user's query. It manages the entire sequence of planning, reasoning, action, evaluation, and final answer extraction to produce a complete response."
      ],
      "metadata": {
        "id": "goGb1KUVmVu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def planner(user_query) -> List[str]:\n",
        "  prompt = f\"\"\"Given the user's query: '{user_query}', break down the query into as few substasks as possible\n",
        "\n",
        "  Each substask should be either a reasoning task or a code generation task. Never duplicate a task.\n",
        "\n",
        "  Here are the only 2 actions that can be taken for each subtask:\n",
        "   - generate_code: this action involves generating python code and executing it in order to make a calculation or verification\n",
        "   - reasoning_ This action involves providing reasong for what to do to complete the subtask\n",
        "\n",
        "  Each subtask should begin with either \"reasoning\" or \"generate_code\".\n",
        "\n",
        "  Keep in mind the overall goal of answering the user's query throughout the planning process\n",
        "\n",
        "  Return the result as a JSON list of strings, where each string is a subtask.\n",
        "\n",
        "  Here is an exmaple JSON response:\n",
        "  {{\n",
        "     \"subtasks\": [\"Subtask 1\", \"Subtask 2\", \"Subtask 3\"]\n",
        "  }}\n",
        "  \"\"\"\n",
        "  response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "  return response[\"subtasks\"]"
      ],
      "metadata": {
        "id": "BpZ1hRaiD-hS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many r's are in the word 'strawberry'?\"\n",
        "subtasks = planner(query)\n",
        "\n",
        "print(subtasks)"
      ],
      "metadata": {
        "id": "MUMjfWwcK7ig",
        "outputId": "ef2a4db2-8568-4f4e-fd1c-24ec6ae26d1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"reasoning: Extract the target word 'strawberry' from the user's query.\", \"reasoning: Identify the specific character 'r' that needs to be counted within the word.\", \"generate_code: Write a function to count the occurrences of the character 'r' in the extracted word using Python.\", 'generate_code: Execute the function to obtain the final count.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reasoner(user_query: str, subtasks: List[str], current_subtask: str) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   Here are all the subtasks to complete in order to answer the user's query:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   - Provide concise reasoning on how to execute the current subtask, considering previous results.\n",
        "   - Prioritize explicit details over assumed patterns\n",
        "   - Avoid unnecessary complications in problem-solving\n",
        "\n",
        "   Return the result as a JSON object with 'reasoning' as a key.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"reasoning\": \"2 sentences max on how to complete the current subtask.\"\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"reasoning\"]\n"
      ],
      "metadata": {
        "id": "BWjY2S2mxh3w"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoner_output = reasoner(query, subtasks, subtasks[2])"
      ],
      "metadata": {
        "id": "1BFyiCpRxh6I",
        "outputId": "3a553f77-9ba4-4ec5-aba0-16fdfba8ce09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Create a Python function named 'count_r' that takes a string as input and returns the count of character 'r'. Iterate through each character in the string using a for loop, checking if the character is equal to 'r', incrementing a counter variable each time. Return the counter variable at the end of the function.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def actioner(user_query: str, subtasks: List[str], current_subtask: str, reasoning: str) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The reasoning for this subtask is:\n",
        "   <reasoning>\n",
        "       {reasoning}\n",
        "   </reasoning>\n",
        "\n",
        "   Determine the most appropriate action to take:\n",
        "       - If the task requires a calculation or verification through code, use the 'generate_code' action.\n",
        "       - If the task requires reasoning without code or calculations, use the 'reasoning' action.\n",
        "\n",
        "   Consider the overall goal and previous results when determining the action.\n",
        "\n",
        "   Return the result as a JSON object with 'action' and 'parameters' keys.  The 'parameters' key should always be a dictionary with 'prompt' as a key.\n",
        "\n",
        "   Example JSON responses:\n",
        "\n",
        "   {{\n",
        "       \"action\": \"generate_code\",\n",
        "       \"parameters\": {{\"prompt\": \"Write a function to calculate the area of a circle.\"}}\n",
        "   }}\n",
        "\n",
        "   {{\n",
        "       \"action\": \"reasoning\",\n",
        "       \"parameters\": {{\"prompt\": \"Explain how to complete the subtask.\"}}\n",
        "   }}\n",
        "   \"\"\"\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n"
      ],
      "metadata": {
        "id": "dE9ZPIpb99ly"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actioner_output = actioner(query, subtasks, subtasks[2], reasoner(query, subtasks, subtasks[2]))"
      ],
      "metadata": {
        "id": "bf-vJJPY99tD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_execute_code(prompt: str, user_query: str) -> Dict[str, Any]:\n",
        "   code_generation_prompt = f\"\"\"\n",
        "\n",
        "   Generate Python code to implement the following task: '{prompt}'\n",
        "\n",
        "   Here is the overall goal of answering the user's query: '{user_query}'\n",
        "\n",
        "   Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "\n",
        "\n",
        "   Here are the guidelines for generating the code:\n",
        "       - Return only the Python code, without any explanations or markdown formatting.\n",
        "       - The code should always print or return a value\n",
        "       - Don't include any backticks or code blocks in your response. Do not include ```python or ``` in your response, just give me the code.\n",
        "       - Do not ever use the input() function in your code, use defined values instead.\n",
        "       - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
        "       - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
        "       - Don't ever provide the execution result in your response, just give me the code.\n",
        "       - If your code needs to import any libraries, do it within the code itself.\n",
        "       - The code should be self-contained and ready to execute on its own.\n",
        "       - Prioritize explicit details over assumed patterns\n",
        "       - Avoid unnecessary complications in problem-solving\n",
        "   \"\"\"\n",
        "\n",
        "   generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "\n",
        "   print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "   old_stdout = sys.stdout\n",
        "   sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "   exec(generated_code)\n",
        "\n",
        "   sys.stdout = old_stdout\n",
        "   output = buffer.getvalue()\n",
        "\n",
        "   print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "   return {\n",
        "       \"generated_code\": generated_code,\n",
        "       \"execution_result\": output.strip()\n",
        "   }\n",
        "\n",
        "\n",
        "def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
        "   if action == \"generate_code\":\n",
        "       print(f\"Generating code for: {parameters['prompt']}\")\n",
        "       return generate_and_execute_code(parameters[\"prompt\"], user_query, memory)\n",
        "   elif action == \"reasoning\":\n",
        "       return parameters[\"prompt\"]\n",
        "   else:\n",
        "       return f\"Action '{action}' not implemented\""
      ],
      "metadata": {
        "id": "jbeS1p8NANBk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "executor(\"generate_code\", parameters=actioner_output['parameters'], user_query = query)"
      ],
      "metadata": {
        "id": "5vmbF-TPBUrT",
        "outputId": "b344057a-102a-4bce-bbd0-4b76e2a3c6ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "executor() missing 1 required positional argument: 'memory'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-0330953d4272>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generate_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactioner_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: executor() missing 1 required positional argument: 'memory'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluator(user_query: str, subtasks: List[str], current_subtask: str, action_info: Dict[str, Any], execution_result: Dict[str, Any], memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks to complete to answer the user's query are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The current subtask to complete is:\n",
        "   <current_subtask>\n",
        "       {current_subtask}\n",
        "   </current_subtask>\n",
        "\n",
        "   The result of the current subtask is:\n",
        "   <result>\n",
        "       {action_info}\n",
        "   </result>\n",
        "\n",
        "   The execution result of the current subtask is:\n",
        "   <execution_result>\n",
        "       {execution_result}\n",
        "   </execution_result>\n",
        "\n",
        "   Here is the short-term memory (result of previous subtasks):\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "\n",
        "   Evaluate if the result is a reasonable answer for the current subtask, and makes sense in the context of the overall query.\n",
        "\n",
        "   Return a JSON object with 'evaluation' (string) and 'retry' (boolean) keys.\n",
        "\n",
        "   Example JSON response:\n",
        "   {{\n",
        "       \"evaluation\": \"The result is a reasonable answer for the current subtask.\",\n",
        "       \"retry\": false\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response\n",
        "\n",
        "\n",
        "def final_answer_extractor(user_query: str, subtasks: List[str], memory: List[Dict[str, Any]]) -> str:\n",
        "   prompt = f\"\"\"Given the user's query (long-term goal): '{user_query}'\n",
        "\n",
        "   The subtasks completed to answer the user's query are:\n",
        "   <subtasks>\n",
        "       {json.dumps(subtasks)}\n",
        "   </subtasks>\n",
        "\n",
        "   The memory of the thought process (short-term memory) is:\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "   Extract the final answer that directly addresses the user's query, from the memory.\n",
        "   Provide only the essential information without unnecessary explanations.\n",
        "\n",
        "   Return a JSON object with 'finalAnswer' as a key.\n",
        "\n",
        "   Here is an example JSON response:\n",
        "   {{\n",
        "       \"finalAnswer\": \"The final answer to the user's query, addressing all aspects of the question, based on the memory provided\",\n",
        "   }}\n",
        "   \"\"\"\n",
        "\n",
        "   response = json.loads(get_llm_response(\"groq\", prompt, json_mode=True))\n",
        "   return response[\"finalAnswer\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_and_execute_code(prompt: str, user_query: str, memory: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "   code_generation_prompt = f\"\"\"\n",
        "\n",
        "   Generate Python code to implement the following task: '{prompt}'\n",
        "\n",
        "   Here is the overall goal of answering the user's query: '{user_query}'\n",
        "\n",
        "   Keep in mind the results of the previous subtasks, and use them to complete the current subtask.\n",
        "   <memory>\n",
        "       {json.dumps(memory)}\n",
        "   </memory>\n",
        "\n",
        "\n",
        "\n",
        "   Here are the guidelines for generating the code:\n",
        "       - Return only the Python code, without any explanations or markdown formatting.\n",
        "       - The code should always print or return a value\n",
        "       - Don't include any backticks or code blocks in your response. Do not include ```python or ``` in your response, just give me the code.\n",
        "       - Do not ever use the input() function in your code, use defined values instead.\n",
        "       - Do not ever use NLP techniques in your code, such as importing nltk, spacy, or any other NLP library.\n",
        "       - Don't ever define a function in your code, just generate the code to execute the subtask.\n",
        "       - Don't ever provide the execution result in your response, just give me the code.\n",
        "       - If your code needs to import any libraries, do it within the code itself.\n",
        "       - The code should be self-contained and ready to execute on its own.\n",
        "       - Prioritize explicit details over assumed patterns\n",
        "       - Avoid unnecessary complications in problem-solving\n",
        "   \"\"\"\n",
        "\n",
        "   generated_code = get_llm_response(\"groq\", code_generation_prompt)\n",
        "\n",
        "\n",
        "   print(f\"\\n\\nGenerated Code: start|{generated_code}|END\\n\\n\")\n",
        "\n",
        "   old_stdout = sys.stdout\n",
        "   sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "   exec(generated_code)\n",
        "\n",
        "   sys.stdout = old_stdout\n",
        "   output = buffer.getvalue()\n",
        "\n",
        "   print(f\"\\n\\n***** Execution Result: |start|{output.strip()}|end| *****\\n\\n\")\n",
        "\n",
        "   return {\n",
        "       \"generated_code\": generated_code,\n",
        "       \"execution_result\": output.strip()\n",
        "   }\n",
        "\n",
        "\n",
        "def executor(action: str, parameters: Dict[str, Any], user_query: str, memory: List[Dict[str, Any]]) -> Any:\n",
        "   if action == \"generate_code\":\n",
        "       print(f\"Generating code for: {parameters['prompt']}\")\n",
        "       return generate_and_execute_code(parameters[\"prompt\"], user_query, memory)\n",
        "   elif action == \"reasoning\":\n",
        "       return parameters[\"prompt\"]\n",
        "   else:\n",
        "       return f\"Action '{action}' not implemented\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def autonomous_agent(user_query: str) -> List[Dict[str, Any]]:\n",
        "   memory = []\n",
        "   subtasks = planner(user_query)\n",
        "\n",
        "   print(\"User Query:\", user_query)\n",
        "   print(f\"Subtasks: {subtasks}\")\n",
        "\n",
        "   for subtask in subtasks:\n",
        "       max_retries = 1\n",
        "       for attempt in range(max_retries):\n",
        "\n",
        "           reasoning = reasoner(user_query, subtasks, subtask, memory)\n",
        "           action_info = actioner(user_query, subtasks, subtask, reasoning, memory)\n",
        "\n",
        "\n",
        "           print(f\"\\n\\n ****** Action Info: {action_info} ****** \\n\\n\")\n",
        "\n",
        "           execution_result = executor(action_info[\"action\"], action_info[\"parameters\"], user_query, memory)\n",
        "\n",
        "           print(f\"\\n\\n ****** Execution Result: {execution_result} ****** \\n\\n\")\n",
        "           evaluation = evaluator(user_query, subtasks, subtask, action_info, execution_result, memory)\n",
        "\n",
        "           step = {\n",
        "               \"subtask\": subtask,\n",
        "               \"reasoning\": reasoning,\n",
        "               \"action\": action_info,\n",
        "               \"evaluation\": evaluation\n",
        "           }\n",
        "           memory.append(step)\n",
        "\n",
        "           print(f\"\\n\\nSTEP: {step}\\n\\n\")\n",
        "\n",
        "           if not evaluation[\"retry\"]:\n",
        "               break\n",
        "\n",
        "           if attempt == max_retries - 1:\n",
        "               print(f\"Max retries reached for subtask: {subtask}\")\n",
        "\n",
        "   final_answer = final_answer_extractor(user_query, subtasks, memory)\n",
        "   return final_answer\n"
      ],
      "metadata": {
        "id": "jS5ph2VfANOm"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "LLuLtjmIDECv",
        "outputId": "48adfc4f-d295-4af1-da25-5c94a58e2f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: How many r's are in the word 'strawberry'?\n",
            "Subtasks: [\"reasoning_Extract the word of interest from the user's query\", 'reasoning_IDentify the specific character(s) being asked about in the query', 'generate_code_Count the occurrences of the character(s) in the extracted word', 'reasoning_Return the calculated count to the user as the final answer']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "reasoner() takes 3 positional arguments but 4 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-760e63c9347d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"How many r's are in the word 'strawberry'?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautonomous_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-da7a5fd03d9d>\u001b[0m in \u001b[0;36mautonomous_agent\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m    148\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m            \u001b[0mreasoning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreasoner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m            \u001b[0maction_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreasoning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: reasoner() takes 3 positional arguments but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ILCIC3vxDENH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IqBVCdODEWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI o1-preview model getting trivial questions wrong\n",
        "\n",
        "- [Link to Reddit post](https://www.reddit.com/r/ChatGPT/comments/1ff9w7y/new_o1_still_fails_miserably_at_trivial_questions/)\n",
        "- Links to ChatGPT threads: [(1)](https://chatgpt.com/share/66f21757-db2c-8012-8b0a-11224aed0c29), [(2)](https://chatgpt.com/share/66e3c1e5-ae00-8007-8820-fee9eb61eae5)\n",
        "- [Improving reasoning in LLMs through thoughtful prompting](https://www.reddit.com/r/singularity/comments/1fdhs2m/did_i_just_fix_the_data_overfitting_problem_in/?share_id=6DsDLJUu1qEx_bsqFDC8a&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)\n"
      ],
      "metadata": {
        "id": "MrSeaoPqq1qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many r's are in the word 'strawberry'?\"\n",
        "autonomous_agent(query)"
      ],
      "metadata": {
        "id": "rM4bcny1JliP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXAPpgkpK7lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjKgUDihI4lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JNPSaGWKF5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ciZ_VUNNJRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Vf9avjo8-fK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}